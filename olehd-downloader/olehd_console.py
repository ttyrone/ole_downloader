# -*- coding: utf-8 -*-
# The MIT License (MIT)
# Copyright (c) 2019 ttyronehank@gmail.com
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE
# OR OTHER DEALINGS IN THE SOFTWARE.

__author__ = 'Tyrone Hank'
__copyright__ = 'Copyright 2021'
__credits__ = ['Lim Kok Hole Links']
__license__ = 'MIT'
__version__ = 1.0
__maintainer__ = 'Tyrone Hank'
__email__ = 'ttyronehank@gmail.com'
__status__ = 'Production'

'''
from slimit import ast
from slimit.parser import Parser
from slimit.visitors import nodevisitor
'''

import sys, os, traceback
import requests
from urllib.parse import urlparse
from olehd_lib.ts_operate import combinets as combinets
from olehd_lib.ts_operate import deletets as deletets
import json
import re 
import subprocess
ffmpeg_path = 'ffmpeg'
try:
    # PyInstaller creates a temp folder and stores path in _MEIPASS
    base_path = sys._MEIPASS
    #print('done base_path: ' + repr(base_path))
except Exception:
    #print('use abs path')
    base_path = os.path.abspath(".")
ffmpeg_full_path = '"' + os.path.join(base_path, os.path.join('olehd_lib', ffmpeg_path) ) + '"'

PY3 = sys.version_info[0] >= 3
if not PY3:
    print('\n[!]ä¸­æ­¢! è¯·ä½¿ç”¨ python 3ã€‚')
    sys.exit(1)

import tqdm
from bs4 import BeautifulSoup

#try: from urllib.request import urlopen #python3
#except ImportError: from urllib2 import urlopen #python2

# RIP UA, https://groups.google.com/a/chromium.org/forum/m/#!msg/blink-dev/-2JIRNMWJ7s/yHe4tQNLCgAJ
UA = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
#from termcolor import cprint
#import colorama
#from colorama import Style, Fore, Back
#colorama.init() # Windows need this
#BOLD_ONLY = ['bold']

# https://github.com/limkokhole/calmjs.parse
import calmjs # Used in `except calmjs...:`
from calmjs.parse import es5
# ~/.local/lib/python3.6/site-packages/calmjs/parse/walkers.py
from calmjs.parse.asttypes import Assign as CalmAssign
from calmjs.parse.asttypes import Identifier as CalmIdentifier
from calmjs.parse.asttypes import String as CalmString
from calmjs.parse.asttypes import VarDecl as CalmVar
from calmjs.parse.walkers import Walker as CalmWalker
from calmjs.parse.parsers.es5 import Parser as CalmParser

from olehd_lib.m3u8_decryptor import main as m3u8_decryptopr_main
from olehd_lib.m3u8_decryptor_export import main as m3u8_decryptopr_export_main
from olehd_lib.ts_operate import fix as mp4_fix
from olehd_lib.ffmpeg_lib import remux_ts_to_mp4 
from olehd_lib.crypto_py_aes import main as crypto_py_aes_main

import logging, http.client
import datetime
import time
import argparse
from argparse import RawTextHelpFormatter
arg_parser = argparse.ArgumentParser(
    description='æ¬§ä¹å½±é™¢ä¸‹è½½å™¨v1.0', formatter_class=RawTextHelpFormatter)

def quit(msgs, exit=True):
    if not isinstance(msgs, list):
        msgs = [msgs]
    # æç¬‘ bug: ä¹‹å‰æ— æ„ä¸­ç§»é™¤ exit=Trueï¼Œå¯¼è‡´æˆ‘ç³»ç»Ÿæœ‰é»˜è®¤ exit æ²¡é—®é¢˜ï¼Œåˆ«äººå´æœ‰é—®é¢˜ã€‚
    if exit: #é¿å…åªçœ‹è§æœ€åä¸€è¡Œâ€œä¸­æ­¢ã€‚â€è€Œä¸æ‡‚å¿…é¡»æ»šä¸ŠæŸ¥çœ‹çœŸæ­£é”™è¯¯åŸå› ã€‚
        msgs[-1]+= 'ä¸­æ­¢ã€‚'
    for msg in msgs:
        if msg == '\n': # Empty line without bg color
            print('\n')
        else:
            #cprint(msg, 'white', 'on_red', attrs=BOLD_ONLY)
            print(msg)
    # Should not do this way, use return instead to support gui callback
    #if exit:
    #    #cprint('Abort.', 'white', 'on_red', attrs=BOLD_ONLY)
    #    sys.exit()


def get_req(url, proxies={}):
    """Get binary data from URL"""
    print('url= '+ url)
    #data = ''
    #or chunk in requests.get(url, headers={'User-agent': 'Mozilla/5.0'}, stream=True):
    #    data += chunk
    init_t = 0.3
    init_a = 0
    total_t = 0
    s = requests.Session()
    success_from_read = False
    while 1:
        try:
            if total_t >= 120:
                return None
            if ( (init_a % 6) == 0) and (init_t < 30):
                init_t+=0.3
            total_t+=init_t
            response =  s.get(url,  proxies=proxies)
            if response.status_code == 200:
                return response.content
            else:
                return None
            #success_from_read = False
        except requests.exceptions.ConnectTimeout:
            init_a+=1
        except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
            success_from_read = True
            for t in (10, 20, 30, 60):
                try:
                    #return s.get(url, headers={'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.0.0 Safari/537.36', 'Referer': referer}, stream=True, timeout=t, proxies=proxies).content
                    response =  s.get(url, proxies=proxies)
                    if response.status_code == 200:
                        return response.content
                    else:
                        return None
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    pass
            return None

    #return requests.get(url, headers={'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.0.0 Safari/537.36'}, stream=False, proxies=proxies).content #tested 1 proxy not able use stream=False
    #if r.status_code == 200:
    #    #r.raw.decode_content = True
    #    #return r.raw
    #    return r
    #return None


#arg_parser.add_argument('-t', '--video-type', dest='video_type', action='store_true', help='Specify movie instead of cinemae')
arg_parser.add_argument('-d', '--dir', help='ç”¨æ¥å‚¨å­˜è¿ç»­å‰§/ç»¼è‰ºçš„ç›®å½•å (éè·¯å¾„)ã€‚')
arg_parser.add_argument('-f', '--file', help='ç”¨æ¥å‚¨å­˜ç”µå½±çš„æ–‡ä»¶åã€‚è¯·åˆ«åŠ åç¼€ .mp4ã€‚')
#from/to options should grey out if -f selected:
arg_parser.add_argument('-da', '--downloadall', help='ä¸‹è½½å…¨é›†')
arg_parser.add_argument('-from-ep', '--from-ep', dest='from_ep', default=1, type=int, help='ä»ç¬¬å‡ é›†å¼€å§‹ä¸‹è½½ã€‚')
arg_parser.add_argument('-to-ep', '--to-ep', dest='to_ep',
                        type=int, help='åˆ°ç¬¬å‡ é›†åœæ­¢ä¸‹è½½ã€‚')
arg_parser.add_argument('-p', '--proxy', help='https ä»£ç†(å¦‚æœ‰)')

arg_parser.add_argument('-g', '--debug', action='store_true', help='å‚¨å­˜ olehd_epN.log æ—¥å¿—é™„ä»¶ï¼Œ ç„¶åä½ å¯ä»¥åœ¨ https://github.com/limkokhole/olehd-downloader/issues æŠ¥å‘Šæ— æ³•ä¸‹è½½çš„é—®é¢˜ã€‚')
arg_parser.add_argument('url', nargs='?', help='ä¸‹è½½é“¾æ¥ã€‚')
args, remaining = arg_parser.parse_known_args()

def main(isMovie, arg_file, arg_url, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy={}, downloadAll=False):

    try:
        sys.stdout = custom_stdout
           
        if not arg_url:
            print('main arg_url: {}'.format(repr(arg_url)))
            return quit('[!] [e1] è¯·ç”¨è¯¥æ ¼å¼  https://www.olehd.com/index.php/vod/detail/id/32220.html çš„é“¾æ¥ã€‚')
        
        parsed_uri=urlparse(arg_url)
        domain = '{uri.netloc}'.format(uri=parsed_uri)
        domain_full = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)


        preresponse = get_req(arg_url, arg_proxy)
        
        if preresponse is not None:
            soup = BeautifulSoup(preresponse,'html.parser')
            ep_name = soup.title.text.split(' - æ¬§ä¹å½±é™¢')[0].split('_')[0]
            try:
                cinema_id = int(arg_url.split('.html')[0].split('/id/')[1])
            except ValueError as ve:
                print(ve)
                #return quit('[!] [e3] Please specify cinema url in https://www.fanstui.com/voddetail-300.html. Abort.')
                return quit('[!] [e3] è¯·ç”¨è¯¥æ ¼å¼  https://www.olehd.com/index.php/vod/detail/id/32220.html çš„é“¾æ¥ã€‚')



            if not isMovie:
                playlist = []
                for ultag in soup.find_all('ul', {'class': 'content_playlist clearfix'}):
                    for litag in ultag.find_all('li'):
                        playlist.append(litag.find('a')['href'])
                if downloadAll:
                    arg_from_ep = 1
                    arg_to_ep = len(playlist)

            arg_file = os.path.abspath(arg_file)

            if not isMovie and not downloadAll:
                for ii in range(arg_from_ep, arg_to_ep+1):
                    try:
                        ep_url= f'{domain_full}{playlist[ii-1]}'
                    except Exception:
                        print('ç¬¬'+str(arg_to_ep)+'é›†ä¸å­˜åœ¨ï¼')
                        continue
                    #print('è¿˜è¦å¤„ç†ep_filenameï¼Œä¸‹è½½ä»¥åçš„åˆå¹¶æ–‡ä»¶åæ˜¯é”™çš„ï¼Œè‡ªåŠ¨ä¿®å¤å’Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶éƒ½ä¸æˆåŠŸ')
                    ep_filename =''
                    try:
                        downloadm4s(isMovie, arg_file, ep_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy, downloadAll)
                        continue
                    except Exception:  
                        print('%s ä¸‹è½½å¤±è´¥ï¼'.format(ep_name))
                        continue
                        #åœ¨playlistä¸­å¾ªç¯å‡ºæ­£ç¡®çš„htmlç»™downloadm4så³å¯
                os.startfile(f'{arg_file}{ep_name}')
            else:
                for ultag in soup.find_all('ul', {'class': 'content_playlist clearfix'}):
                    for litag in ultag.find_all('li'):
                        #ç”µå½±æ’­æ”¾é¡µé¢çš„url
                        ep_filename = ''
                        url_path = litag.find('a')['href']
                        ep_url= f'{domain_full}{url_path}'
                        if litag.find('a').string != 'åœ¨çº¿æ’­æ”¾' and litag.find('a').string != 'é«˜æ¸…æ’­æ”¾':
                            ep_filename =litag.find('a').string
                        #å¾ªç¯ä¸‹è½½init.mp4å’Œm4s
                        #D:\é•¿æ´¥æ¹–\é•¿æ´¥æ¹– ä¸Šé›†_fix.mp4
                                            
                        linksy = '\\'
                        tmp = f'{arg_file}{ep_name}{linksy}{ep_name} {ep_filename}{"_fix.mp4"}'
                        if not os.path.isfile(tmp):
                            downloadm4s(isMovie, arg_file, ep_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy, downloadAll)
                        else:
                            print(f'{ ep_name} {ep_filename}{"å·²ä¸‹è½½ï¼"}')
                            continue
                os.startfile(f'{arg_file}{ep_name}')
        else:
            return quit('[i] è¯·æ±‚å¤±è´¥ï¼Œè¯·é‡è¯•ï¼ ')
    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')
    
    
    try:
        print('[ğŸ˜„] å…¨éƒ¨ä¸‹è½½å·¥ä½œå®Œæ¯•ã€‚æ‚¨å·²å¯ä»¥å…³é—­çª—å£, æˆ–ä¸‹è½½åˆ«çš„è§†é¢‘ã€‚')
    except UnicodeEncodeError:
        print('[*] å…¨éƒ¨ä¸‹è½½å·¥ä½œå®Œæ¯•ã€‚æ‚¨å·²å¯ä»¥å…³é—­çª—å£, æˆ–ä¸‹è½½åˆ«çš„è§†é¢‘ã€‚')

#ep_name é•¿æ´¥æ¹–
#ep_filename ä¸Šé›†ï¼Œå¦‚æœåªæœ‰ä¸€é›†çš„å°±æ˜¯ç©º
def downloadm4s(isMovie, arg_file, arg_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy={}, downloadAll=False):
    try:
        arg_file = f'{arg_file}{ep_name}'

        if '://' not in arg_url:
            arg_url = 'https://' + arg_url
            print('arg_url= '+ arg_url)
        dir_path_m = os.path.abspath(arg_file)
        if not os.path.isdir(dir_path_m):
            try:
                os.makedirs(dir_path_m)
            except OSError:
                return quit('[i] æ— æ³•åˆ›å»ºç›®å½•ã€‚æˆ–è®¸å·²æœ‰åŒåæ–‡ä»¶ï¼Ÿ ')

        if isMovie:
            json_path= dir_path_m + "\\" + ep_name + ' ' + ep_filename + " import.json"
        else:
            json_path= dir_path_m + "\\" + ep_name + ' ' + ep_filename + str(arg_from_ep)+'-'+str(arg_to_ep)  + " import.json"

        if os.path.isfile(json_path):
            os.remove(json_path)


        # https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python
        http_headers = {
            'User-Agent': UA,
            'method': 'GET',
            'scheme': 'https',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }

        def calm_assign(node):
            return isinstance(node, CalmAssign)

        def calm_id(node):
            return isinstance(node, CalmIdentifier)

        def calm_str(node):
            return isinstance(node, CalmString)

        def calm_var(node):
            return isinstance(node, CalmVar)

        CP = CalmParser()
        walker = CalmWalker()
        
        
        # è·å–ç¬¬ä¸€ä¸ªm3u8æ–‡ä»¶

        print('[...] å°è¯• URL: ' + arg_url)
        
        try:
            init_t = 1
            init_a = 0
            total_t = 0
            s = requests.Session()
            success_from_read = False
            while 1:
                try:
                    if total_t >= 120:
                        return None
                    if ( (init_a % 6) == 0) and (init_t < 30):
                        init_t+=1
                    total_t+=init_t
                    r = s.get(arg_url, allow_redirects=True, headers=http_headers, timeout=init_t, proxies=arg_proxy)
                    #success_from_read = False
                    break
                except requests.exceptions.ConnectTimeout:
                    init_a+=1
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    success_from_read = True
                    break
            if success_from_read: # since we use Session(), so can reuse the "connected" session!
                try:
                    r = s.get(arg_url, allow_redirects=True, headers=http_headers, timeout=30, proxies=arg_proxy)
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    print('[!A] å¤±è´¥ã€‚')

        except requests.exceptions.ConnectionError:
            print('\n[!] ä½ çš„ç½‘ç»œå‡ºç°é—®é¢˜ï¼Œä¹Ÿå¯èƒ½æ˜¯ç½‘ç«™çš„æœåŠ¡å™¨é—®é¢˜ã€‚\n', flush=True)

        soup = BeautifulSoup(r.text, 'html.parser')
        if ep_filename =='':
            ep_filename = soup.title.text.split(' - æ¬§ä¹å½±é™¢')[0].split('_')[1]
            if os.path.isfile(dir_path_m+'\\' + ep_name + ' ' + ep_filename + '_fix.mp4'):
                print (ep_name + ' ' + ep_filename+'å·²ä¸‹è½½ï¼ï¼')
                return

        try:
            pattern = re.compile(r"var player_aaaa=", re.MULTILINE | re.DOTALL) 
            script = soup.find("script", text=pattern) 
            m3u8_url = json.loads(script.text.replace('var player_aaaa=',''))["url"]#ç¬¬ä¸€ä¸ªm3u8æ–‡ä»¶
        except IndexError:
            print('è·å–m3u8æ–‡ä»¶å¤±è´¥: ')
            
        ct_b64 = '' #reset
        passwd = '' #reset

        printed_err = False
        got_ep_url = False

        

        if m3u8_url and dir_path_m:
            got_ep_url = True
            print('m3u8å¤´æ–‡ä»¶ url: ' + m3u8_url)
            print('ä¸‹è½½çš„ mp4 è·¯å¾„: ' + dir_path_m)

            init_t = 1
            init_a = 0
            total_t = 0
            s = requests.Session()
            success_from_read = False
            while 1:
                try:
                    if total_t >= 120:
                        return None
                    if ( (init_a % 6) == 0) and (init_t < 30):
                        init_t+=1
                    total_t+=init_t
                    r = s.get(m3u8_url, allow_redirects=True, headers=http_headers, timeout=init_t, proxies=arg_proxy)
                    #success_from_read = False
                    break
                except requests.exceptions.ConnectTimeout:
                    init_a+=1
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    success_from_read = True
                    break
            if success_from_read: # since we use Session(), so can reuse the "connected" session!
                try:
                    r = s.get(m3u8_url, allow_redirects=True, headers=http_headers, timeout=30, proxies=arg_proxy)
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    print('[!C] å¤±è´¥ã€‚')
            
            #if isMovie:#ç”µå½±
            if not os.path.isfile(dir_path_m+'\\' + ep_name + ' ' + ep_filename + '_fix.mp4'):
                #å°†çœŸå®init.mp4å’Œm4sæ–‡ä»¶çš„åœ°å€ä¼ è¿‡å»ä¸‹è½½
                if m3u8_decryptopr_main(isMovie, r.text, dir_path_m,  ep_name, ep_filename, arg_from_ep, arg_to_ep,  m3u8_url, http_headers, proxies=arg_proxy):
                    if not os.path.isfile(dir_path_m+ "\\" + ep_name + ' ' + ep_filename +" import.json"):
                        #remux_ts_to_mp4(ep_ts_path, ep_mp4_path)
                        if combinets(isMovie, ep_name, ep_filename, dir_path_m, ep_name):#print('åˆå¹¶æˆMP4')åŒºåˆ†ç”µå½±å’Œç”µè§†å‰§
                            if mp4_fix(isMovie, ffmpeg_full_path,ep_name, ep_filename, dir_path_m, ep_name) :
                                print('[!] {}ä¿®å¤MP4å®Œæ¯•ï¼Œ'.format(ep_name ))
                            else:
                                print('[!] {}ä¿®å¤MP4å¤±è´¥ï¼Œä¸å½±å“è§‚çœ‹ï¼Œä½†æ— æ³•æ‹–åŠ¨ï¼Œä»‹æ„è¯·è‡ªè¡Œç”¨å‘½ä»¤ï¼šffmpeg -i ä¸‹è½½å¾…ä¿®å¤.mp4 -c copy ä¸‹è½½å¾…ä¿®å¤_fix.mp4ã€‚'.format(ep_name ))
                            if not deletets(isMovie, ep_name, ep_filename, dir_path_m, ep_name):#print('å¦‚æœåˆå¹¶æˆåŠŸå°±åˆ é™¤ä¸´æ—¶æ–‡ä»¶')
                                print('[!] {}åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ï¼Œå‘½ä»¤ï¼šdel {}*.ts æˆ–åœ¨èµ„æºç®¡ç†å™¨ä¸­é€‰æ‹©åˆ é™¤ã€‚'.format(dir_path_m, ep_name))
                            else:
                                print('[!] {}åˆ é™¤ä¸´æ—¶æ–‡ä»¶å®Œæ¯•ã€‚'.format(dir_path_m, ep_name))
                        else:
                            print('[!] {}åˆå¹¶MP4å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åˆå¹¶ï¼Œå‘½ä»¤ï¼šcopy /b {}*.m4s xxx.mp4ã€‚'.format(dir_path_m, ep_name))
                    else:
                        file = open(json_path,encoding='utf-8', errors='ignore')
                        file_lines = file.read()
                        file.close()
                        file_json = json.loads(file_lines)
                        n = len(file_json['item'])
                        print('{} {} å…±æœ‰{}ä¸ªä¸‹è½½é”™è¯¯çš„tsæ–‡ä»¶éœ€è¦é‡æ–°ä¸‹è½½ï¼Œè¯·å°†{}å¯¼å…¥åˆ°postmanï¼Œé€ä¸ªä¸‹è½½æ–‡ä»¶ï¼Œå¹¶å‘½åä¸ºè¯·æ±‚å.tsï¼ˆå¦‚ï¼šè¯·æ±‚åä¸ºç¬¬xxé›†-002ï¼Œä¸‹è½½æ–‡ä»¶å°±å‘½åä¸ºç¬¬xxé›†-002.tsï¼‰ï¼Œç„¶åå°†æ‰€æœ‰ä¸‹è½½æ–‡ä»¶æ‹·å›åˆ°{}å¹¶æ›¿æ¢åŸæœ‰æ–‡ä»¶ï¼Œç„¶åæŒ‰åç§°é¡ºåºæ’åºï¼Œè¿›è¡Œåˆå¹¶å³å¯ï¼ˆåˆå¹¶å‘½ä»¤ï¼šcopy /b {}*.m4s {}.mp4ï¼‰ï¼Œåˆå¹¶åè¯·è‡ªè¡Œåˆ é™¤ç›¸å…³çš„tsæ–‡ä»¶'.format(ep_name, ep_filename, str(n), json_path, json_path, ep_filename, ep_filename))
                    #æ‰“å¼€ä¸‹è½½ç›®å½•
                    
                else: 
                    print('[!] {} tsæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œè¯·å†è¯•ä¸€æ¬¡ã€‚'.format(ep_name + ' ' + ep_filename))
            else:
                print('%s å·²ä¸‹è½½' %(ep_name + ' ' + ep_filename ))
            

            #source_url = "https://tv2.xboku.com/20191126/wNiFeUIj/index.m3u8"
            #https://stackoverflow.com/questions/52736897/custom-user-agent-in-youtube-dl-python-script
            #youtube_dl.utils.std_headers['User-Agent'] = UA
            #try: # This one shouldn't pass .mp4 ep_path
            #    youtube_dl.YoutubeDL(params={'-c': '', '-q': '', '--no-mtime': '',
            #                                 'outtmpl': ep_path + '.%(ext)s'}).download([ep_url])
            #except youtube_dl.utils.DownloadError:
            #    print(traceback.format_exc())
            #    print(
            #        'Possible reason is filename too long. Please retry with -s <maximum filename size>.')
            #    sys.exit()

        #print(walker.extract(tree, assignment))


        if not got_ep_url:
            if not printed_err:
                if arg_file:
                    print('[!] ä¸å­˜åœ¨è¯¥éƒ¨å½±ç‰‡ã€‚')
                else:
                    print('[!] ä¸å­˜åœ¨è¯¥éƒ¨å½±ç‰‡ã€‚')

    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')



def export(isMovie, arg_file, arg_url, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy={}, downloadAll=False):

    try:
        sys.stdout = custom_stdout
           
        if not arg_url:
            print('main arg_url: {}'.format(repr(arg_url)))
            return quit('[!] [e1] è¯·ç”¨è¯¥æ ¼å¼  https://www.olehd.com/index.php/vod/detail/id/32220.html çš„é“¾æ¥ã€‚')
        
        parsed_uri=urlparse(arg_url)
        domain = '{uri.netloc}'.format(uri=parsed_uri)
        domain_full = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)


        preresponse = get_req(arg_url, arg_proxy)
        
        if preresponse is not None:
            soup = BeautifulSoup(preresponse,'html.parser')
            ep_name = soup.title.text.split(' - æ¬§ä¹å½±é™¢')[0].split('_')[0]
            try:
                cinema_id = int(arg_url.split('.html')[0].split('/id/')[1])
            except ValueError as ve:
                print(ve)
                #return quit('[!] [e3] Please specify cinema url in https://www.fanstui.com/voddetail-300.html. Abort.')
                return quit('[!] [e3] è¯·ç”¨è¯¥æ ¼å¼  https://www.olehd.com/index.php/vod/detail/id/32220.html çš„é“¾æ¥ã€‚')



            if not isMovie:
                if downloadAll:
                    playlist = []
                    for ultag in soup.find_all('ul', {'class': 'content_playlist clearfix'}):
                        for litag in ultag.find_all('li'):
                            playlist.append(litag.find('a')['href'])
                    arg_from_ep = 1
                    arg_to_ep = len(playlist)

            arg_file = os.path.abspath(arg_file)

            if not isMovie:
                playlist = []
                for ultag in soup.find_all('ul', {'class': 'content_playlist clearfix'}):
                    for litag in ultag.find_all('li'):
                        playlist.append(litag.find('a')['href'])
                if downloadAll:
                    arg_from_ep = 1
                    arg_to_ep = len(playlist)

            arg_file = os.path.abspath(arg_file)

            if not isMovie and not downloadAll:
                for ii in range(arg_from_ep, arg_to_ep+1):
                    try:
                        ep_url= f'{domain_full}{playlist[ii-1]}'
                    except Exception:
                        print('ç¬¬'+str(arg_to_ep)+'é›†ä¸å­˜åœ¨ï¼')
                        continue
                    #print('è¿˜è¦å¤„ç†ep_filenameï¼Œä¸‹è½½ä»¥åçš„åˆå¹¶æ–‡ä»¶åæ˜¯é”™çš„ï¼Œè‡ªåŠ¨ä¿®å¤å’Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶éƒ½ä¸æˆåŠŸ')
                    ep_filename =''
                    try:
                        exportm4s(isMovie, arg_file, ep_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy, downloadAll)
                        continue
                    except Exception:  
                        print('%s ä¸‹è½½å¤±è´¥ï¼'.format(ep_name))
                        continue
                        #åœ¨playlistä¸­å¾ªç¯å‡ºæ­£ç¡®çš„htmlç»™downloadm4så³å¯
                os.startfile(f'{arg_file}{ep_name}')
            else:
                for ultag in soup.find_all('ul', {'class': 'content_playlist clearfix'}):
                    for litag in ultag.find_all('li'):
                        #ç”µå½±æ’­æ”¾é¡µé¢çš„url
                        ep_filename = ''
                        url_path = litag.find('a')['href']
                        ep_url= f'{domain_full}{url_path}'
                        if litag.find('a').string != 'åœ¨çº¿æ’­æ”¾' and litag.find('a').string != 'é«˜æ¸…æ’­æ”¾':
                            ep_filename =litag.find('a').string
                        #å¾ªç¯ä¸‹è½½init.mp4å’Œm4s
                        #D:\é•¿æ´¥æ¹–\é•¿æ´¥æ¹– ä¸Šé›†_fix.mp4
                        linksy = '\\'
                        tmp = f'{arg_file}{ep_name}{linksy}{ep_name} {ep_filename}{"-download_list.txt"}'
                        if not os.path.isfile(tmp):
                            exportm4s(isMovie, arg_file, ep_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy, downloadAll)
                        else:
                            print(f'{ ep_name} {ep_filename}{"å·²ä¸‹è½½ï¼"}')
                            continue
            os.startfile(f'{arg_file}{ep_name}')
        else:
            return quit('[i] è¯·æ±‚å¤±è´¥ï¼Œè¯·é‡è¯•ï¼ ')
    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')
    
    
    try:
        print('[ğŸ˜„] å…¨éƒ¨å¯¼å‡ºå·¥ä½œå®Œæ¯•ã€‚æ‚¨å·²å¯ä»¥å¯¼å…¥å…¶ä»–ä¸‹è½½è½¯ä»¶è¿›è¡Œä¸‹è½½ã€‚')
    except UnicodeEncodeError:
        print('[*] å…¨éƒ¨å¯¼å‡ºå·¥ä½œå®Œæ¯•ã€‚æ‚¨å·²å¯ä»¥å¯¼å…¥å…¶ä»–ä¸‹è½½è½¯ä»¶è¿›è¡Œä¸‹è½½ã€‚')

            
def exportm4s(isMovie, arg_file, arg_url, ep_name, ep_filename, arg_from_ep, arg_to_ep, custom_stdout, arg_debug, arg_proxy={}, downloadAll=False):
    try:
        arg_file = f'{arg_file}{ep_name}'

        if '://' not in arg_url:
            arg_url = 'https://' + arg_url
            print('arg_url= '+ arg_url)
        dir_path_m = os.path.abspath(arg_file)
        if not os.path.isdir(dir_path_m):
            try:
                os.makedirs(dir_path_m)
            except OSError:
                return quit('[i] æ— æ³•åˆ›å»ºç›®å½•ã€‚æˆ–è®¸å·²æœ‰åŒåæ–‡ä»¶ï¼Ÿ ')

        if isMovie:
            json_path= dir_path_m + "\\" + ep_name + ' ' + ep_filename + "-download_list.txt"
        else:
            json_path= dir_path_m + "\\" + ep_name + ' ' + ep_filename + str(arg_from_ep)+'-'+str(arg_to_ep)  + "-download_list.txt"

        if os.path.isfile(json_path):
            os.remove(json_path)


        # https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python
        http_headers = {
            'User-Agent': UA,
            'method': 'GET',
            'scheme': 'https',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }

        def calm_assign(node):
            return isinstance(node, CalmAssign)

        def calm_id(node):
            return isinstance(node, CalmIdentifier)

        def calm_str(node):
            return isinstance(node, CalmString)

        def calm_var(node):
            return isinstance(node, CalmVar)

        CP = CalmParser()
        walker = CalmWalker()
        
        
        # è·å–ç¬¬ä¸€ä¸ªm3u8æ–‡ä»¶

        print('[...] å°è¯• URL: ' + arg_url)
        
        try:
            init_t = 1
            init_a = 0
            total_t = 0
            s = requests.Session()
            success_from_read = False
            while 1:
                try:
                    if total_t >= 120:
                        return None
                    if ( (init_a % 6) == 0) and (init_t < 30):
                        init_t+=1
                    total_t+=init_t
                    r = s.get(arg_url, allow_redirects=True, headers=http_headers, timeout=init_t, proxies=arg_proxy)
                    #success_from_read = False
                    break
                except requests.exceptions.ConnectTimeout:
                    init_a+=1
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    success_from_read = True
                    break
            if success_from_read: # since we use Session(), so can reuse the "connected" session!
                try:
                    r = s.get(arg_url, allow_redirects=True, headers=http_headers, timeout=30, proxies=arg_proxy)
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    print('[!A] å¤±è´¥ã€‚')

        except requests.exceptions.ConnectionError:
            print('\n[!] ä½ çš„ç½‘ç»œå‡ºç°é—®é¢˜ï¼Œä¹Ÿå¯èƒ½æ˜¯ç½‘ç«™çš„æœåŠ¡å™¨é—®é¢˜ã€‚\n', flush=True)

        soup = BeautifulSoup(r.text, 'html.parser')

        if ep_filename =='':
            ep_filename = soup.title.text.split(' - æ¬§ä¹å½±é™¢')[0].split('_')[1]
            if os.path.isfile(dir_path_m+'\\' + ep_name + ' ' + ep_filename + str(arg_from_ep)+'-'+str(arg_to_ep)  + '-download_list.txt'):
                print (ep_name + ' ' + ep_filename+'å·²å¯¼å‡ºä¸‹è½½åˆ—è¡¨ï¼ï¼')
                return
        try:
            pattern = re.compile(r"var player_aaaa=", re.MULTILINE | re.DOTALL) 
            script = soup.find("script", text=pattern) 
            m3u8_url = json.loads(script.text.replace('var player_aaaa=',''))["url"]#ç¬¬ä¸€ä¸ªm3u8æ–‡ä»¶
        except IndexError:
            print('è·å–m3u8æ–‡ä»¶å¤±è´¥: ')
            
        ct_b64 = '' #reset
        passwd = '' #reset

        printed_err = False
        got_ep_url = False

        

        if m3u8_url and dir_path_m:
            got_ep_url = True
            print('m3u8å¤´æ–‡ä»¶ url: ' + m3u8_url)
            print('ä¸‹è½½çš„ mp4 è·¯å¾„: ' + dir_path_m)

            init_t = 1
            init_a = 0
            total_t = 0
            s = requests.Session()
            success_from_read = False
            while 1:
                try:
                    if total_t >= 120:
                        return None
                    if ( (init_a % 6) == 0) and (init_t < 30):
                        init_t+=1
                    total_t+=init_t
                    r = s.get(m3u8_url, allow_redirects=True, headers=http_headers, timeout=init_t, proxies=arg_proxy)
                    #success_from_read = False
                    break
                except requests.exceptions.ConnectTimeout:
                    init_a+=1
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    success_from_read = True
                    break
            if success_from_read: # since we use Session(), so can reuse the "connected" session!
                try:
                    r = s.get(m3u8_url, allow_redirects=True, headers=http_headers, timeout=30, proxies=arg_proxy)
                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                    print('[!C] å¤±è´¥ã€‚')
            
            if not os.path.isfile(json_path):
                #å°†çœŸå®init.mp4å’Œm4sæ–‡ä»¶çš„åœ°å€ä¼ è¿‡å»ä¸‹è½½
                if m3u8_decryptopr_export_main(isMovie, r.text, dir_path_m, ep_name, ep_filename, m3u8_url, http_headers, proxies=arg_proxy):
                    print('%s ä¸‹è½½åˆ—è¡¨å·²å¯¼å‡º' %(ep_name + ' ' + ep_filename ))
                else: 
                    print('[!] {} ä¸‹è½½åˆ—è¡¨å¯¼å‡ºå¤±è´¥ï¼Œè¯·å†è¯•ä¸€æ¬¡ã€‚'.format(ep_name + ' ' + ep_filename))
            else:
                print('%s ä¸‹è½½åˆ—è¡¨å·²å¯¼å‡º' %(ep_name + ' ' + ep_filename ))
            

            #source_url = "https://tv2.xboku.com/20191126/wNiFeUIj/index.m3u8"
            #https://stackoverflow.com/questions/52736897/custom-user-agent-in-youtube-dl-python-script
            #youtube_dl.utils.std_headers['User-Agent'] = UA
            #try: # This one shouldn't pass .mp4 ep_path
            #    youtube_dl.YoutubeDL(params={'-c': '', '-q': '', '--no-mtime': '',
            #                                 'outtmpl': ep_path + '.%(ext)s'}).download([ep_url])
            #except youtube_dl.utils.DownloadError:
            #    print(traceback.format_exc())
            #    print(
            #        'Possible reason is filename too long. Please retry with -s <maximum filename size>.')
            #    sys.exit()

        #print(walker.extract(tree, assignment))


        if not got_ep_url:
            if not printed_err:
                if arg_file:
                    print('[!] ä¸å­˜åœ¨è¯¥éƒ¨å½±ç‰‡ã€‚')
                else:
                    print('[!] ä¸å­˜åœ¨è¯¥éƒ¨å½±ç‰‡ã€‚')

    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')


def combine(isMovie, arg_file, custom_stdout):

    try:
        sys.stdout = custom_stdout
        
        file_path = os.path.abspath(arg_file)
        '''if not os.path.isfile(arg_file):
            print('[!] [e1] æ‰€é€‰è·¯å¾„ä¸å­˜åœ¨ã€‚')
            return quit('[!] [e1] æ‰€é€‰è·¯å¾„ä¸å­˜åœ¨ã€‚')'''

        
        dir_arr =[]

        for root, dirs, files in os.walk(file_path):
            #print('root_dir:', root)  # å½“å‰ç›®å½•è·¯å¾„
            #print('sub_dirs:', dirs)  # å½“å‰è·¯å¾„ä¸‹æ‰€æœ‰å­ç›®å½•
            #print('files:', files)  # å½“å‰è·¯å¾„ä¸‹æ‰€æœ‰éç›®å½•å­æ–‡ä»¶
            for i in dirs:
                dir_arr.append(i)

        for dirname in dir_arr:
            all_ts = os.listdir(arg_file+'/'+dirname)

        #å¾ªç¯å–å¾—æ‰€æœ‰çš„æ–‡ä»¶å¤¹å
        
            temp_ts = []
            init_mp4 = ''
            for j in all_ts:
                if os.path.splitext(j)[1].endswith('.m4s'):
                    temp_ts.append(j)
            for k in all_ts:
                if os.path.splitext(k)[1].endswith('.mp4'):
                    init_mp4= k
                    break

            needed_ts = []

            temp_ts.sort(key=lambda x: int(x.split('seg-')[1].split('-v1-a1.m4s')[0]))

            for i in temp_ts:
                needed_ts.append(i)

            needed_ts.insert(0,init_mp4)

            #print(time.mktime(datetime.datetime.now().timetuple()))
            hebing_path = f'{arg_file}{"/"}{dirname}{"/"}{dirname}{".mp4"}'
            try:
                if not os.path.isfile(hebing_path):
                    with open(hebing_path, 'wb+') as f:
                        for ii in range(len(needed_ts)):
                            ts_video_path = os.path.join(arg_file+'/'+dirname, needed_ts[ii])
                            f.write(open(ts_video_path, 'rb').read())
                    print("%s åˆå¹¶å®Œæˆï¼Œå³å°†åˆ é™¤ä¸‹è½½çš„m4sæ–‡ä»¶ï¼ï¼" %(dirname+'.mp4'))
                    try:
                        if os.path.exists(arg_file+'/'+dirname):
                            os.chdir(arg_file+'/'+dirname)
                            file_list = os.listdir()
                            for file in file_list:
                                if file.startswith('seg-') and file.endswith('.m4s'):
                                    os.remove(arg_file+'/'+dirname+'/'+file)
                            os.remove(arg_file+'/'+dirname+'/init-v1-a1.mp4')
                            print(" %s ä¸´æ—¶æ–‡ä»¶åˆ é™¤å®Œæ¯•ï¼" %(arg_file+'/'+dirname ))
                            continue
                    except Exception as e:
                        print("%s ä¸´æ—¶æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ï¼ï¼" %(dirname))
                        continue
                    continue
                else:
                    continue
            except Exception as e:
                print("%s åˆå¹¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åˆå¹¶ï¼ï¼" %(dirname+'.mp4'))
                continue
                #ä¸éœ€è¦ä¿®å¤ä¹Ÿèƒ½æ‹–åŠ¨ï¼Œï¼Œ
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œ




        
        
  
    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')

    try:
        print('[ğŸ˜„] å…¨éƒ¨åˆå¹¶å·¥ä½œå®Œæ¯•ã€‚')
    except UnicodeEncodeError:
        print('[*] å…¨éƒ¨åˆå¹¶å·¥ä½œå®Œæ¯•ã€‚')




def fixfile(arg_file):

    try:
        if not arg_file:
            print('å¾…ä¿®å¤è§†é¢‘: {}'.format(repr(arg_file)))
            return quit('[!] [e1] è¯·é€‰æ‹©è¦ä¿®å¤çš„è§†é¢‘ã€‚')
        
        if not os.path.isfile(arg_file):
            print('è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {}'.format(repr(arg_file)))
            return quit('[!] [e1] è¯·é€‰æ‹©è¦ä¿®å¤çš„è§†é¢‘ã€‚')
        
        print('å³å°†å¼€å§‹ä¿®å¤è§†é¢‘æ–‡ä»¶ï¼š%sã€‚' %(arg_file))

        filearr = arg_file.split('/')
        count = len(filearr)-1
        filename= filearr[count]
        download_path =''
        for index in range(count):
             download_path += filearr[index]+'/'


        new_filename = filename.split('.mp4')[0]+'_fix.mp4'
        new_path = '"'+download_path+new_filename+'"'
        arg_file_final = '"'+arg_file+'"'
        if os.path.isfile(new_path):
            print('è§†é¢‘æ–‡ä»¶å·²ä¿®å¤: {}ï¼Œæ— é¡»é‡å¤ä¿®å¤ã€‚'.format(repr(new_path)))
        else:
            #ffmpeg -i 2.mp4 -vcodec copy -strict -2 video_out.mp4
            fix_cmd = f'{ffmpeg_full_path} {"-i"} {arg_file_final} {"-c copy"} {new_path}'
            try:
                result=subprocess.Popen(fix_cmd,shell=False)
                return_code=result.wait()
                if return_code==0:
                    print('[ğŸ˜„] ä¿®å¤å®Œæ¯•ã€‚')
                    os.remove(arg_file)
                else:
                    print('[!] å‡ºç°é”™è¯¯ã€‚')
            except Exception as e:
                print('[!] å‡ºç°é”™è¯¯ã€‚')
        
        os.startfile(download_path)

    except Exception:
        try:
            print(traceback.format_exc())
        except UnicodeEncodeError:
            print('[!] å‡ºç°é”™è¯¯ã€‚')


    '''
    #slimit, https://stackoverflow.com/questions/44503833/python-slimit-minimizer-unwanted-warning-output
    parser = Parser()
    tree = parser.parse(script.text)
    for node in nodevisitor.visit(tree):
        if isinstance(node, ast.Assign):
            print(666)
    '''

if __name__ == "__main__":
    main(args.dir, args.file, args.from_ep, args.to_ep, args.url, sys.stdout, args.debug, args.proxy, args.proxy_local, args.downloadAll)
